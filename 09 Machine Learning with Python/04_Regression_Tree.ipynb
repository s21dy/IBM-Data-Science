{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d55c553",
   "metadata": {},
   "source": [
    "# Regression Trees\n",
    "\n",
    "As we have already seen, decision trees can be used for classification, but we can also use them for regression, commonly called regression trees. \n",
    "\n",
    "The basic idea behind regression trees is to split our data into groups based on features, like in classification, and return a prediction that is the average across the data we have already seen. \n",
    "\n",
    "Consider the housing data below, where we are using the ‘Age’ to predict the ‘Price’ of a house. \n",
    "\n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/wFQ7z7rdQ1aUO8-63WNWXQ_47a05259a036469ebdd262ca669e9af1_image001.png?expiry=1718409600000&hmac=7ZmHI49OEQJjT8W4fW1FkEiVTUP042wM_jBJZZErpzE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bfcb3",
   "metadata": {},
   "source": [
    "Here, we can see the difference age has on the house prices. Ages between 0 and 10 have an average price of approximately $500,000, ages between 10 and 50 have an average price of approximately $380,000 and houses older than 50 years have an average price of approximately $100,000. Using these general ranges, we can predict the price of a house. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0e624",
   "metadata": {},
   "source": [
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/6w9F__-yRYaPRf__smWGGA_5c92e4cb24494b679947474912ba6df1_image002.png?expiry=1718409600000&hmac=hIE43HxhtHPa6hk-ACgEfC_GI4WcVS9WYDApjSo0Dy8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14ae9e",
   "metadata": {},
   "source": [
    "Using the data above, we can create the regression tree, as shown below. The prices were determined by calculating the average price of the houses in the age range. \n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/blVYMSJdS-iVWDEiXWvofw_0e4d4fa2159b4037a47a174d9bd3f8f1_image003.png?expiry=1718409600000&hmac=7R64dQLScIzALoLGmwDeGhlb23cK9s5qIEweK9yWWkE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d687930",
   "metadata": {},
   "source": [
    "## Criterion\n",
    "\n",
    "The way the trees are built are similar to classification, but instead of using the entropy criterion. In Classification Trees, we choose features that increase the information gain. In Regression Trees, we choose features that minimize the error. \n",
    "\n",
    "A popular one is the Mean Absolute Error.\n",
    "\n",
    "### How Regression Trees are Built? \n",
    "Take the dataset sample shown below, the first step is to decide what the first decision is. We will do this by using the criterion and checking every single feature in the dataset to see which one produces the minimal error. \n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/aAggxRhNTfyIIMUYTS38_Q_147311cc36a04061b0c8e211a3f92ff1_image006.png?expiry=1718409600000&hmac=5G-G8yyaA3jPp-mEW_d0smmJkCBkD06T3L0w2dD5F1E)\n",
    "\n",
    "### Categorical Features \n",
    "Categorical features are simple, here we have Near Water so all we need to do is calculate the error if we used this as the first feature. Near Water feature has two categories: ‘Yes’ and ‘No’, therefore, we must calculate the average ‘Price’ of houses in the ‘Yes’ and ‘No’ categories. Then we use those values to calculate the average error. \n",
    "\n",
    "### ‘No’ Category:\n",
    "- Index of Houses in ‘No’ Category = [0, 1, 2, 3, 4]\n",
    "- Prices of Houses in ‘No’ Category = [260831.34, 222939.35, 101882.1, 226868.52, 94868.94] \n",
    "- Average House Price in ‘No’ Category = 181478.05\n",
    "- Absolute Error = [79353.29, 41461.30, 79595.95, 45390.47, 86609.11]\n",
    "\n",
    "### ‘Yes’ Category:\n",
    "- Index of Houses in ‘Yes’ Category = [5,6,7,8,9]\n",
    "- Prices of Houses in ‘Yes’ Category = [197703.55, 347982.98, 343150.38, 206713.16, 329768.77] \n",
    "- Average House Price in ‘Yes’ Category = 285063.77\n",
    "- Absolute Error = [87360.22, 62919.22, 58086.61, 78350.61, 44705.00] \n",
    "\n",
    "### MAE \n",
    "This is the MAE of the Near Water feature, and this number will be used to compare against MAE of all other features to determine which one is the lowest. Therefore, it will establish the first decision in our regression tree. \n",
    "\n",
    "MAE = 66383.17   \n",
    "\n",
    "### Numerical Features \n",
    "Numerical features, like ‘Age’, are trickier to handle because we need to find a number, instead of using a category, to split the data by. We do this by creating a boundary between each point, then we calculate the error. \n",
    "\n",
    "For example, first we create the boundary between the first two data points, which are (0, 260831.34) and (5, 347982.98), so we create a boundary of x = 2.5 (The midpoint between the x component of the first two data points). We now find the average price of the houses on the left and right sides of this boundary and use it to calculate the MAE.\n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/2ZbQVjP2SjCW0FYz9oowgA_7f1075ff6d71438cbe267ff95a678bf1_image007.png?expiry=1718409600000&hmac=TpYG6l8Z-SOvzAJFavK5bbhezDAI-S7rDdCxkZoNc9w)\n",
    "\n",
    "### Left: \n",
    "Index of Houses on the Left Side = [0]\n",
    "Prices of Houses on the Left Side = [260831.34] Average House Price of Left Side = 260831.34 Absolute Error = [0]\n",
    "\n",
    "### Right: \n",
    "Index of Houses on the Right Side = [1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
    "Prices of Houses in the Right Category = [222939.35, 101882.1, 226868.52, 94868.94, 197703.55, 347982.98, 343150.38, 206713.16, 329768.77] \n",
    "Average House Price in the Right Category = 230208.64 \n",
    "Absolute Error = [7269.29, 128326.54, 3340.12, 135339.7, 32505.09, 117774.34, 112941.74, 23495.48, 99560.13] \n",
    "\n",
    "### MAE \n",
    "Now we can find the MAE, using the absolute errors from the left and right sides. \n",
    "\n",
    "MAE = 66055.24 \n",
    "\n",
    "### Further Steps \n",
    "This process is then repeated for each boundary between each pair of consecutive points. \n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bdN5rzBkRxSTea8wZBcUOA_2ca24feedfac41df96359b36a03acdf1_image008.png?expiry=1718409600000&hmac=N-TVOWTGo42W1Wi4F-B-D9KtuzitmDNqpHiA22KnQzM)\n",
    "\n",
    "## Choosing the Decision \n",
    "Now, we compare the categorical MAE and the lowest numerical MAE, in this case, the categorical is 66055.24, and the numerical is 49726.55. So, for the first decision, we will use the numerical ‘Age’ feature. We end up with a regression tree that looks like this:\n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0LIZQg7VQoqyGUIO1fKKfA_f24263d5e1e14fac8a0b34dde1d018f1_image010.png?expiry=1718409600000&hmac=S6PKGcVxj0yzQPHAjezYrXaUcS3TmxqPEMSWlJRdsnA)\n",
    "\n",
    "### When do we Stop? \n",
    "With the regression tree above, we have two options, we can either stop here and use the average value of the ‘Yes’ (left) and ‘No’ (right) to predict the house prices, or we can continue to add more decisions to either branch. There are a few conditions that are commonly used to stop growing regression trees: \n",
    "\n",
    "- Tree depth\n",
    "- Number of remaining samples on a branch\n",
    "- Number of samples on each branch if another decision is made \n",
    "\n",
    "The depth of the tree above, is 1, because there is a single decision and the number of samples on each side is 5. Let’s add more decisions until the depth of the tree is 2. First, we start with the ‘Yes’ (left) side and we calculate the MAE for the features using the houses that have ‘Age’ < 35.\n",
    "\n",
    "### Adding Decisions\n",
    "- Left\n",
    "    Like before, we use the Near Water feature and calculate the MAE on houses with index 0, 3, 6, 7, and 9.\n",
    "\n",
    "- Categorical Features\n",
    "    MAE = 11005.34\n",
    "\n",
    "- Numerical Features\n",
    "    Now, we find the MAE for the boundaries in the ‘Age’ feature.\n",
    "    \n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/PCMuGvmEQ4mjLhr5hKOJTQ_ac28f321feeb4a95b6824444582665f1_image011.png?expiry=1718409600000&hmac=Jm1k91XK8iSAUpo1LIUChv7eBHhyPuKCsF8-PJiYv1E)\n",
    "\n",
    "## Final Result\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0c572bDITRSOe9mwyL0UfA_3ad1d65e6505446fabf21569e3b041f1_image013.png?expiry=1718409600000&hmac=NzmKc0kiPOfMrwmIEK8miG7yVVvReVQIl1_XIwD3PBQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af76b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas will allow us to create a dataframe of the data so it can be used and manipulated\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dead1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/real_estate_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac7df44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "\n",
       "   LSTAT  MEDV  \n",
       "0   4.98  24.0  \n",
       "1   9.14  21.6  \n",
       "2   4.03  34.7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dacb09ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       20\n",
       "ZN         20\n",
       "INDUS      20\n",
       "CHAS       20\n",
       "NOX         0\n",
       "RM          0\n",
       "AGE        20\n",
       "DIS         0\n",
       "RAD         0\n",
       "TAX         0\n",
       "PTRATIO     0\n",
       "LSTAT      20\n",
       "MEDV        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d000d9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b4d10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adf82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dependent Variable (Y) and independent Variable (X)\n",
    "X = data.drop(columns=[\"MEDV\"])\n",
    "Y = data[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a50186",
   "metadata": {},
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=1)\n",
    "Regression Trees are implemented using `DecisionTreeRegressor` from `sklearn.tree`\n",
    "\n",
    "The important parameters of `DecisionTreeRegressor` are\n",
    "\n",
    "`criterion`: {\"mse\", \"friedman_mse\", \"mae\", \"poisson\"} - The function used to measure error\n",
    "\n",
    "`max_depth` - The max depth the tree can be\n",
    "\n",
    "`min_samples_split` - The minimum number of samples required to split a node\n",
    "\n",
    "`min_samples_leaf` - The minimum number of samples that a leaf can contain\n",
    "\n",
    "`max_features`: {\"auto\", \"sqrt\", \"log2\"} - The number of feature we examine looking for the best one, used to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f73f52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Regression Tree\n",
    "regression_tree = DecisionTreeRegressor(criterion = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56447b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 3027.8481012658226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:359: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "regression_tree.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluation\n",
    "regression_tree.score(X_test, Y_test)\n",
    "\n",
    "# Find the average error in our testing set \n",
    "# which is the average error in median home value prediction\n",
    "prediction = regression_tree.predict(X_test)\n",
    "print(\"$\",(prediction - Y_test).abs().mean()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9941a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1fde6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
