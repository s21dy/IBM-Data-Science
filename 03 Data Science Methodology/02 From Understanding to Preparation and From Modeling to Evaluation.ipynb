{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bdef26",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Purpose:\n",
    "- To determine if the collected data is representative of the problem to be solved.\n",
    "- Involves constructing the data set and analyzing its characteristics.\n",
    "\n",
    "- Case Study Application\n",
    "    - Descriptive Statistics:\n",
    "        - Run against data columns to become variables in the model.\n",
    "        - Includes measures like mean, median, minimum, maximum, and standard deviation.\n",
    "    - Pairwise Correlations:\n",
    "        - Identify relationships between variables.\n",
    "        - Determine if any variables are highly correlated and redundant.\n",
    "    - Histograms:\n",
    "        - Examine distributions of variables.\n",
    "        - Help decide on data preparation steps, such as consolidating categorical values.\n",
    "    - Data Quality Assessment\n",
    "        - Re-coding or Dropping Values:\n",
    "            - Address missing or invalid values.\n",
    "            - Example: Numeric variable \"age\" with values 0 to 100 and 999, where 999 means \"missing\".\n",
    "    - Iterative Process\n",
    "        - Refining Definitions:\n",
    "            - Initial definition of congestive heart failure admission based on primary diagnosis.\n",
    "            - Data understanding revealed the need to include secondary and tertiary diagnoses.\n",
    "            - Loop back to data collection to refine the definition and improve the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d08d8",
   "metadata": {},
   "source": [
    "# Data Preparation - Concepts\n",
    "\n",
    "- Data Preparation\n",
    "- Analogy:\n",
    "    - Similar to washing vegetables to remove unwanted elements.\n",
    "    - Essential for transforming data into a usable state.\n",
    "\n",
    "- Time Consumption\n",
    "    - Significance:\n",
    "        - Data preparation, along with data collection and understanding, is the most time-consuming phase, taking 70% to 90% of the project time.\n",
    "        - Automation can reduce this time to as little as 50%, allowing more focus on model creation.\n",
    "\n",
    "- Data Transformation\n",
    "    - Cooking Metaphor:\n",
    "        - Like chopping onions finely to enhance flavor distribution in a sauce.\n",
    "        - Transforming data makes it easier to work with.\n",
    "\n",
    "- Key Questions\n",
    "    - Objective:\n",
    "        - Address missing or invalid values, remove duplicates, and ensure proper formatting.\n",
    "        - Feature engineering to create useful characteristics for machine learning algorithms.\n",
    "\n",
    "- Feature Engineering\n",
    "    - Importance:\n",
    "        - Uses domain knowledge to create features that improve model performance.\n",
    "        - Critical for predictive models and influences results.\n",
    "\n",
    "- Text Analysis\n",
    "    - Steps:\n",
    "        - Coding data for manipulation.\n",
    "        - Ensuring proper groupings and not overlooking hidden information.\n",
    "\n",
    "- Importance of Data Preparation\n",
    "    - Foundation:\n",
    "        - Sets the stage for subsequent steps.\n",
    "        - If done correctly, supports the project; if skipped, may lead to subpar outcomes and rework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b17db",
   "metadata": {},
   "source": [
    "# Data Preparation - Case Study\n",
    "\n",
    "## Case Study: Data Preparation\n",
    "\n",
    "### Defining Congestive Heart Failure:\n",
    "- **Initial Step**: Define congestive heart failure precisely.\n",
    "- **Diagnosis Codes**: Identify diagnosis-related group codes for fluid buildup and heart failure types.\n",
    "- **Clinical Guidance**: Needed to get the correct codes.\n",
    "\n",
    "### Defining Readmission Criteria:\n",
    "- **Timing of Events**: Evaluate to define initial (index) admission vs. readmission.\n",
    "- **30-Day Window**: Set as the readmission period following discharge.\n",
    "\n",
    "### Aggregating Transactional Records:\n",
    "- **Data Format**: Multiple records per patient, including claims and clinical services.\n",
    "- **Aggregation**: Combine records to create a single record per patient for modeling.\n",
    "- **New Columns**: Created to represent information such as visit frequency and co-morbidities.\n",
    "\n",
    "### Considering Co-morbidities:\n",
    "- Examples: Diabetes, hypertension, and other chronic conditions impacting readmission risk.\n",
    "\n",
    "### Literary Review:\n",
    "- **Purpose**: Ensure no important data elements were overlooked.\n",
    "- **Outcome**: Added more indicators for conditions and procedures.\n",
    "\n",
    "### Final Data Aggregation:\n",
    "- **Merging Data**: Combine transactional data with demographic information.\n",
    "- **Patient Table**: One record per patient with many columns representing attributes.\n",
    "\n",
    "### Variables for Modeling:\n",
    "- Dependent Variable: Congestive heart failure readmission within 30 days (yes/no).\n",
    "\n",
    "### Cohort Creation:\n",
    "- Size: 2,343 patients meeting criteria.\n",
    "- Split: Divided into training and testing sets for model building and validation.\n",
    "\n",
    "# Summary\n",
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eSgbfmUivhLEYkKzQI0izQ.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22278405",
   "metadata": {},
   "source": [
    "# Glossary: From Understanding to Preparation\n",
    "\n",
    "| Term | Definition |\n",
    "|:-:|:---|\n",
    "| Analytics team | A group of professionals, including data scientists and analysts, responsible for performing data analysis and modeling. |\n",
    "| Data collection | The process of gathering data from various sources, including demographic, clinical, coverage, and pharmaceutical information. |\n",
    "| Data integration | The merging of data from multiple sources to remove redundancy and prepare it for further analysis. |\n",
    "| Data Preparation | The process of organizing and formatting data to meet the requirements of the modeling technique. |\n",
    "| Data Requirements | The identification and definition of the necessary data elements, formats, and sources required for analysis. |\n",
    "| Data Understanding | A stage where data scientists discuss various ways to manage data effectively, including automating certain processes in the database. |\n",
    "| DBAs (Database Administrators) | The professionals who are responsible for managing and extracting data from databases. |\n",
    "| Decision tree classification | A modeling technique that uses a tree-like structure to classify data based on specific conditions and variables. |\n",
    "| Demographic information | Information about patient characteristics, such as age, gender, and location. |\n",
    "| Descriptive statistics | Techniques used to analyze and summarize data, providing initial insights and identifying gaps in data. |\n",
    "| Intermediate results | Partial results obtained from predictive modeling can influence decisions on acquiring additional data. |\n",
    "| Patient cohort | A group of patients with specific criteria selected for analysis in a study or model. |\n",
    "| Predictive modeling | The building of models to predict future outcomes based on historical data. |\n",
    "| Training set | A subset of data used to train or fit a machine learning model; consists of input data and corresponding known or labeled output values. |\n",
    "| Unavailable data | Data elements are not currently accessible or integrated into the data sources. |\n",
    "| Univariate | Modeling analysis focused on a single variable or feature at a time, considering its characteristics and relationship to other variables independently. |\n",
    "| Unstructured data | Data that does not have a predefined structure or format, typically text images, audio, or video, requires special techniques to extract meaning or insights. |\n",
    "| Visualization | The process of representing data visually to gain insights into its content and quality. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b337b34",
   "metadata": {},
   "source": [
    "# From Modeling to Evaluation\n",
    "\n",
    "### Purpose of Data Modeling:\n",
    "- Develop models that are either descriptive or predictive.\n",
    "\n",
    "- Characteristics of the Process:\n",
    "    -  Descriptive models: Examine patterns (e.g., if a person did this, theyâ€™re likely to prefer that).\n",
    "    - Predictive models: Yield yes/no or stop/go outcomes.\n",
    "\n",
    "- Analytical Approach\n",
    "    - Types:\n",
    "        - Statistically driven or machine learning driven.\n",
    "    - Training Set:\n",
    "        - Historical data with known outcomes.\n",
    "        - Used to calibrate the model.\n",
    "\n",
    "- Model Development\n",
    "    - Algorithm Testing:\n",
    "        - Experiment with different algorithms to ensure necessary variables are included.\n",
    "    - Success Factors:\n",
    "        - Understanding the problem.\n",
    "        - Appropriate analytical approach.\n",
    "        - Quality of data (like ingredients in cooking).\n",
    "        \n",
    "- Continuous Improvement\n",
    "    - Refinement:\n",
    "        - Constant adjustments and tweaking are necessary to ensure a solid outcome.\n",
    "        \n",
    "### John Rollins' Descriptive Data Science Methodology\n",
    "- Framework Goals:\n",
    "    - Understand the question at hand.\n",
    "    - Select an analytic approach or method to solve the problem.\n",
    "    - Obtain, understand, prepare, and model the data.\n",
    "- End Goal: Build a data model to answer the question.\n",
    "\n",
    "- Evaluation and Deployment\n",
    "    - Key Question:\n",
    "        - Have I made enough to eat? (Metaphor for ensuring the model is sufficient).\n",
    "- Relevance:\n",
    "    - Model evaluation, deployment, and feedback loops ensure the answer is relevant.\n",
    "    - Critical for the development of the data science field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408e9c6",
   "metadata": {},
   "source": [
    "## Modeling Stage - Case Study\n",
    "\n",
    "### Initial Model\n",
    "- First decision tree classification model built for congestive heart failure readmission\n",
    "    - Looking for patients with high-risk readmission (outcome = \"yes\")\n",
    "\n",
    "- Initial model accuracy:\n",
    "    - Overall accuracy = 85%\n",
    "    - Accuracy for \"yes\" outcomes = 45% (low)\n",
    "\n",
    "### Improving Model Accuracy\n",
    "   - Question: How to improve accuracy in predicting \"yes\" outcomes?\n",
    "   - For decision trees, adjust the relative cost of misclassified \"yes\" and \"no\" outcomes\n",
    "\n",
    "### Cost of Misclassification\n",
    "   - False Positive (Type I error): True non-readmission misclassified as readmission\n",
    "        - Cost = Wasted intervention\n",
    "   - False Negative (Type II error): True readmission misclassified as non-readmission\n",
    "        - Cost = Readmission costs + patient trauma (higher cost)\n",
    "\n",
    "### Parameter Tuning\n",
    "   - Default relative cost weight = 1:1 for \"yes\" and \"no\"\n",
    "   - Model 2: Relative cost set to 9:1 (favoring \"yes\" outcomes)\n",
    "        - \"Yes\" accuracy = 97%, but low \"no\" accuracy (49% overall)\n",
    "        - Too many false positives, not a good model\n",
    "   - Model 3: Relative cost set to 4:1\n",
    "       - \"Yes\" sensitivity = 68%, \"No\" specificity = 85%, overall 81%\n",
    "       - Best balance with small training set\n",
    "\n",
    "### Further Refinement\n",
    "   - More iterations back to data preparation stage\n",
    "   - Redefine variables to better represent underlying information\n",
    "   - Improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2b92a",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "### Importance of Model Evaluation\n",
    "- Purpose:\n",
    "    - Assess the quality of the model.\n",
    "    - Ensure it meets the initial request.\n",
    "- Key Question:\n",
    "    - Does the model answer the initial question or need adjustments?\n",
    "\n",
    "### Phases of Model Evaluation\n",
    "   1. Diagnostic Measures Phase:\n",
    "       - Ensures the model works as intended.\n",
    "       - Predictive Models: Use decision trees to evaluate alignment with initial design.\n",
    "       - Descriptive Models: Apply a testing set with known outcomes for refinement.\n",
    "    2. Statistical Significance Testing:\n",
    "        - Ensures proper handling and interpretation of data.\n",
    "        - Avoids unnecessary second-guessing when revealing answers.\n",
    "\n",
    "## Case Study Application\n",
    "- Parameter Tuning:\n",
    "    - Example: Tuning the relative cost of misclassifying \"yes\" and \"no\" outcomes.\n",
    "    - Four models built with different relative misclassification costs.\n",
    "    - Observation: Increasing true-positive rate (sensitivity) at the expense of false-positive rate.\n",
    "\n",
    "- Optimal Model Selection\n",
    "    - ROC Curve:\n",
    "        - Definition: Receiver Operating Characteristic curve.\n",
    "        - Purpose: Quantifies binary classification model performance.\n",
    "        - Plot: True Positive Rate (TPR) vs. False Positive Rate (FPR) for different misclassification costs.\n",
    "        - Optimal Model: Maximum separation between ROC curve and baseline.\n",
    "        - Example: Model 3 with a 4-to-1 misclassification cost was optimal.\n",
    "\n",
    "\n",
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-DS0103EN-Coursera/images/Model%20to%20Evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b868a73",
   "metadata": {},
   "source": [
    "# Glossary\n",
    "\n",
    "| Term | Definition |\n",
    "|:-:|:---|\n",
    "| Automation | Using tools and techniques to streamline data collection and preparation processes. |\n",
    "| Data Collection | The phase of gathering and assembling data from various sources. |\n",
    "| Data Compilation | The process of organizing and structuring data to create a comprehensive data set. |\n",
    "| Data Formatting | The process of standardizing the data to ensure uniformity and ease of analysis. |\n",
    "| Data Manipulation | The process of transforming data into a usable format. |\n",
    "| Data Preparation | The phase where data is cleaned, transformed, and formatted for further analysis, including feature engineering and text analysis. |\n",
    "| Data Preparation | The stage where data is transformed and organized to facilitate effective analysis and modeling. |\n",
    "| Data Quality | Assessment of data integrity and completeness, addressing missing, invalid, or misleading values. |\n",
    "| Data Quality Assessment | The evaluation of data integrity, accuracy, and completeness. |\n",
    "| Data Set | A collection of data used for analysis and modeling. |\n",
    "| Data Understanding | The stage in the data science methodology focused on exploring and analyzing the collected data to ensure that the data is representative of the problem to be solved. |\n",
    "| Descriptive Statistics | Summary statistics that data scientists use to describe and understand the distribution of variables, such as mean, median, minimum, maximum, and standard deviation. |\n",
    "| Feature | A characteristic or attribute within the data that helps in solving the problem. |\n",
    "| Feature Engineering | The process of creating new features or variables based on domain knowledge to improve machine learning algorithms' performance. |\n",
    "| Feature Extraction | Identifying and selecting relevant features or attributes from the data set. |\n",
    "| Interactive Processes | Iterative and continuous refinement of the methodology based on insights and feedback from data analysis. |\n",
    "| Missing Values | Values that are absent or unknown in the dataset, requiring careful handling during data preparation. |\n",
    "| Model Calibration | Adjusting model parameters to improve accuracy and alignment with the initial design. |\n",
    "| Pairwise Correlations | An analysis to determine the relationships and correlations between different variables. |\n",
    "| Text Analysis | Steps to analyze and manipulate textual data, extracting meaningful information and patterns. |\n",
    "| Text Analysis Groupings | Creating meaningful groupings and categories from textual data for analysis. |\n",
    "| Visualization techniques | Methods and tools that data scientists use to create visual representations or graphics that enhance the accessibility and understanding of data patterns, relationships, and insights. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ebb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
